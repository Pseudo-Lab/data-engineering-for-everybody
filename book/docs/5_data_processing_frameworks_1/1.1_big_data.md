# 1.1 대규모 데이터 처리 필요성과 프레임워크

**📌 대규모 데이터의 기준**

- 기존의 데이터 처리방법으로는 감당하기 힘들 정도로 방대한 분량의 데이터
- 수십 TB ~ 수십 PB

**📌 이 대규모 데이터를 처리하기 위해 필요한 프레임워크로는 어떤 것들이 있을까?**

- Hadoop
    - 대용량의 데이터를 분산 처리
    - HDFS + MapReduce
        - HDFS: 분산 **저장** 기술
        - MapReduce: 분산 **처리** 기술
- Spark
    - **실시간성 데이터에 대한 니즈가 증가**하며 **하둡으로 처리하기에는 속도 측면에서 부적합**한 시나리오들이 등장하기 시작
    - **분산 시스템을 사용한 프로그래밍 환경**으로 **대량의 메모리를 활용하여 고속화** 실현
    - **인메모리상**에서 동작하기 때문에, **반복적인 처리가 필요한 작업에서 속도가 하둡보다 최소 1000배 이상 빠르다.**
    - 스파크는 Hadoop 이 아니라 MapReduce 를 대체하는 기술
- Hive
    - **HDFS등에 있는 파일을 읽어들여 쿼리로 분석을 수행**
    - Hive는 데이터베이스가 아닌 **데이터 처리를 위한 배치 처리 구조**이다.
    - 대규모 정형 데이터를 처리할 때 매우 안정적
    

**🤔 Spark 와 Hive 둘 중 어떤 걸 사용해야 할까?**

- Spark의 경우 **실시간 데이터 처리와 데이터 과학 등의 분석 작업**에 매우 적합합니다.
    - 빠른 처리 속도, 병렬 처리를 위해 분산 컴퓨팅 환경에서 실행
    - 다양한 언어 지원
- Hive의 경우 대용량의 정형 데이터를 처리하기에 적합하며, 데이터 마이닝 및 비즈니스 인텔리전스 작업에 적합합니다.
    - SQL 기반의 쿼리 언어 지원
    - 데이터를 읽고 쓰는 데 많은 시간이 소요되지만, 대규모 데이터를 처리하는 데 효율적 → **안정적**