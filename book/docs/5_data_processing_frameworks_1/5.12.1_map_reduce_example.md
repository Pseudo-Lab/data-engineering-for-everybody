# ğŸ’» MapReduce ì‹¤ìŠµ

## 1) Docker ì„¤ì¹˜
* MacOS ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

```bash
brew install cask
```

```bash
brew install docker --cask
```

## 2) HDFS/Spark Workbench Quick Start

> [https://github.com/big-data-europe/docker-hadoop-spark-workbench](https://github.com/big-data-europe/docker-hadoop-spark-workbench)

- Container ë„ìš°ê¸°
    - Hadoop, Sparkê¹Œì§€ ëª¨ë‘ ë„ìš°ê³  ì‹¶ë‹¤ë©´
        
        ```bash
        docker-compose up -d
        ```
        
    - Hadoopë§Œ ë„ìš°ê³  ì‹¶ë‹¤ë©´
        
        ```bash
        docker-compose -f docker-compose-hive.yml up -d namenode
        docker-compose -f docker-compose-hive.yml up -d datanode
        ```
        
- Container í™•ì¸

```bash
docker ps
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-04 á„‹á…©á„’á…® 4.06.26.png](images/2.5.1_docker_ps.png)

- Hadoop check

```bash
docker exec -it namenode /bin/bash
```

```bash
hadoop fs -ls /
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-04 á„‹á…©á„’á…® 4.07.24.png](images/2.5.2_hadoop_ls.png)

(ì²« ì„¤ì¹˜ í›„ì—ëŠ” ì•„ë¬´ ê²ƒë„ ì—†ëŠ”ê²Œ ì •ìƒì…ë‹ˆë‹¤!)

## 3) MapReduce ì‹¤ìŠµ

MapReduce ì‹¤ìŠµìœ¼ë¡œ ë§ì´ í™œìš©ë˜ëŠ” WordCountë¥¼ ì§„í–‰í•´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤. WordCountëŠ” ë§ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì„¸ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì™œ Hadoop ì‹¤ìŠµìœ¼ë¡œ WordCountë¥¼ ì§„í–‰í• ê¹Œìš”? Hadoopì€ ê¸°ë³¸ì ìœ¼ë¡œ ë¶„ì‚° ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë¶„ì‚° ì €ì¥ëœ txt íŒŒì¼ì„ ê°ê°ì˜ nodeì—ì„œ WordCountë¥¼ ì§„í–‰í•œ í›„ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê²Œ ë©ë‹ˆë‹¤. ë¬¼ë¡  ì´ë²ˆ ì‹¤ìŠµì—ì„  datanodeë¥¼ í•˜ë‚˜ë§Œ ë„ì› ê¸° ë•Œë¬¸ì— ì§„ì •í•œ ì˜ë¯¸ì˜ ë¶„ì‚° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆœ ì—†ìœ¼ë‚˜ Hadoopì˜ êµ¬ì¡°ì™€ MapReduceì˜ ì‘ë™ ë°©ì‹ì„ ì´í•´í•˜ëŠ”ë°ëŠ” ë¬´ë¦¬ê°€ ì—†ì„ ê²ƒì…ë‹ˆë‹¤. 

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-04 á„‹á…©á„’á…® 4.16.35.png](images/2.5.3_yml.png)

ë¨¼ì € ì•ì„œ êµ¬ì„±í•œ hadoop í´ëŸ¬ìŠ¤í„°ì˜ datanodeì™€ namenodeëŠ” local machineì˜ volumesê³¼ ì—°ê²°ì„ í•´ë‘ì—ˆëŠ”ë°ìš”, yml íŒŒì¼ì—ì„œ volume êµ¬ì„±ì´ `./data/datanode:/hadoop/dfs/data` ì´ë ‡ê²Œ ë˜ì–´ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” localì—ì„œ `./data/datanode`ì˜ ìœ„ì¹˜ì™€ ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ `/hadoop/dfs/data`ì˜ ê²½ë¡œë¥¼ ì—°ë™ì‹œí‚¨ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì €í¬ëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ `cd ./data/datanode`ë¥¼ í†µí•´ ìœ„ì¹˜ë¥¼ ì´ë™í•´ì¤ë‹ˆë‹¤. 

ì´í›„ ì €í¬ê°€ ì‹¤ìŠµì— ì‚¬ìš©í•  íŒŒì¼ì„ í•˜ë‚˜ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤!

- `words.txt`
    
    ```
    DE4E ìŠ¤í„°ë”” ê°€ì§œì—°êµ¬ì†Œ ë°ì—” ìŠ¤í„°ë”” ëª¨ì„
    ê°€ì§œì—°êµ¬ì†Œ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ ì¤‘ì‹¬ ëª¨ì„
    ```
    

ì´ì œ MapReduce ì½”ë“œë¥¼ ì‘ì„±í•  ì°¨ë¡€ì…ë‹ˆë‹¤.

![https://docs.google.com/presentation/d/1nIMJN3m9n9EEDtTSa_rRYwyAKlNUtQg6KSHTOfAVD_A/edit#slide=id.p](images/2.5.4_mapreduce_example.png)

[https://docs.google.com/presentation/d/1nIMJN3m9n9EEDtTSa_rRYwyAKlNUtQg6KSHTOfAVD_A/edit#slide=id.p](https://docs.google.com/presentation/d/1nIMJN3m9n9EEDtTSa_rRYwyAKlNUtQg6KSHTOfAVD_A/edit#slide=id.p)

- `WordCount.java` (Apache Hadoop ê³µì‹ Tutorial)
    
    ```java
    import java.io.IOException;
    import java.util.StringTokenizer;
    
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    public class WordCount {
    
      // Mapper í´ë˜ìŠ¤ ì •ì˜
      public static class TokenizerMapper
           extends Mapper<Object, Text, Text, IntWritable>{
    
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
    
        // Map ë©”ì„œë“œ êµ¬í˜„
        public void map(Object key, Text value, Context context
                        ) throws IOException, InterruptedException {
          StringTokenizer itr = new StringTokenizer(value.toString());  // ì…ë ¥ ë°ì´í„°ë¥¼ í† í°í™”
          while (itr.hasMoreTokens()) {  // í† í°ì´ ì¡´ì¬í•˜ëŠ” ë™ì•ˆ ë°˜ë³µ
            word.set(itr.nextToken());  // í† í°ì„ Text ê°ì²´ì— ì €ì¥
            context.write(word, one);  // ì¶œë ¥ í‚¤/ê°’ ìŒ ìƒì„± í›„ ì¶œë ¥
          }
        }
      }
    
      // Reducer í´ë˜ìŠ¤ ì •ì˜
      public static class IntSumReducer
           extends Reducer<Text,IntWritable,Text,IntWritable> {
        private IntWritable result = new IntWritable();
    
        // Reduce ë©”ì„œë“œ êµ¬í˜„
        public void reduce(Text key, Iterable<IntWritable> values,
                           Context context
                           ) throws IOException, InterruptedException {
          int sum = 0;
          for (IntWritable val : values) {  // ë™ì¼í•œ í‚¤ë¥¼ ê°€ì§„ ê°’ë“¤ì„ í•©ì‚°
            sum += val.get();
          }
          result.set(sum);  // í•©ì‚° ê²°ê³¼ë¥¼ IntWritable ê°ì²´ì— ì €ì¥
          context.write(key, result);  // ì¶œë ¥ í‚¤/ê°’ ìŒ ìƒì„± í›„ ì¶œë ¥
        }
      }
    
      public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();  // Hadoop ì„¤ì • ì •ë³´ ê°ì²´ ìƒì„±
        
        Job job = Job.getInstance(conf, "word count");  // Job ê°ì²´ ìƒì„±
        job.setJarByClass(WordCount.class);  // Job í´ë˜ìŠ¤ ì„¤ì •
        job.setMapperClass(TokenizerMapper.class);  // Mapper í´ë˜ìŠ¤ ì„¤ì •
        job.setCombinerClass(IntSumReducer.class);  // Combiner í´ë˜ìŠ¤ ì„¤ì •
        job.setReducerClass(IntSumReducer.class);  // Reducer í´ë˜ìŠ¤ ì„¤ì •
        job.setOutputKeyClass(Text.class);  // ì¶œë ¥ í‚¤ í´ë˜ìŠ¤ ì„¤ì •
        job.setOutputValueClass(IntWritable.class);  // ì¶œë ¥ ê°’ í´ë˜ìŠ¤ ì„¤ì •
        
        FileInputFormat.addInputPath(job, new Path(args[0]));  // ì…ë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        FileOutputFormat.setOutputPath(job, new Path(args[1]));  // ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        
        System.exit(job.waitForCompletion(true) ? 0 : 1);  // Job ì‹¤í–‰ í›„ ì¢…ë£Œ
      }
    }
    ```
    
    - ë” ìì„¸íˆ
        
        ```java
         private final static IntWritable one = new IntWritable(1);
        ```
        
        ì´ ì½”ë“œëŠ” IntWritable íƒ€ì…ì˜ ë³€ìˆ˜ oneì„ ìƒì„±í•˜ê³ , ì´ ë³€ìˆ˜ì˜ ì´ˆê¸°ê°’ì„ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. IntWritableì€ Hadoopì—ì„œ ì‚¬ìš©ë˜ëŠ” ì •ìˆ˜í˜• íƒ€ì…ì´ë©°, MapReduce ì‘ì—…ì—ì„œ ì¶œë ¥í•  ê°’ì„ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ì½”ë“œëŠ” WordCount ì˜ˆì œì—ì„œ Mapper í´ë˜ìŠ¤ì—ì„œ ì¶œë ¥í•  ê°’ì˜ ì´ˆê¸°ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. Mapper í´ë˜ìŠ¤ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ê° ë‹¨ì–´ë³„ë¡œ (ë‹¨ì–´, 1)ì˜ í˜•íƒœë¡œ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë•Œ 1ì´ë¼ëŠ” ê°’ì„ IntWritable íƒ€ì…ì˜ ë³€ìˆ˜ oneì— ì €ì¥í•˜ê³ , ì´ ë³€ìˆ˜ë¥¼ ì¶œë ¥ ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¦‰, Mapperì—ì„œ ìƒì„±ë˜ëŠ” ì¶œë ¥ ê°’ì€ (ë‹¨ì–´, one)ì˜ í˜•íƒœë¡œ ìƒì„±ë©ë‹ˆë‹¤.
        
        ```java
         // Map ë©”ì„œë“œ êµ¬í˜„
            public void map(Object key, Text value, Context context
                            ) throws IOException, InterruptedException {
              StringTokenizer itr = new StringTokenizer(value.toString());  // ì…ë ¥ ë°ì´í„°ë¥¼ í† í°í™”
              while (itr.hasMoreTokens()) {  // í† í°ì´ ì¡´ì¬í•˜ëŠ” ë™ì•ˆ ë°˜ë³µ
                word.set(itr.nextToken());  // í† í°ì„ Text ê°ì²´ì— ì €ì¥
                context.write(word, one);  // ì¶œë ¥ í‚¤/ê°’ ìŒ ìƒì„± í›„ ì¶œë ¥
              }
            }
        ```
        
        ìœ„ ì½”ë“œëŠ” Mapper í´ë˜ìŠ¤ì˜ map ë©”ì„œë“œë¥¼ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.
        
        ì´ ë©”ì„œë“œëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ í† í°í™”í•˜ì—¬ í† í°ì´ ì¡´ì¬í•˜ëŠ” ë™ì•ˆ í† í°ì„ í•˜ë‚˜ì”© ê°€ì ¸ì™€ì„œ í‚¤-ê°’ ìŒì„ ìƒì„±í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.
        
        ì—¬ê¸°ì„œëŠ” ì…ë ¥ ê°’ìœ¼ë¡œ ë“¤ì–´ì˜¨ Text ê°ì²´ë¥¼ StringTokenizer í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•˜ê³ , whileë¬¸ì„ ì´ìš©í•˜ì—¬ í† í°ì´ ì¡´ì¬í•˜ëŠ” ë™ì•ˆ ë°˜ë³µì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        ê·¸ë¦¬ê³  ê°ê°ì˜ í† í°ì„ word ë³€ìˆ˜ì— ì €ì¥í•œ í›„, context.write ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ í‚¤-ê°’ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì´ë•Œ, ì¶œë ¥ ê°’ìœ¼ë¡œëŠ” Mapper í´ë˜ìŠ¤ ë‚´ë¶€ì— ë¯¸ë¦¬ ì„ ì–¸ëœ one ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë³€ìˆ˜ëŠ” IntWritable í´ë˜ìŠ¤ íƒ€ì…ì˜ ê°ì²´ë¡œ, ê·¸ ê°’ì€ 1ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë“  ì¶œë ¥ ê°’ì€ 1ì´ ë©ë‹ˆë‹¤.
        

ì´ì œ datanodeì— ì ‘ì†í•´ì„œ ì•ì„œ ë§Œë“¤ì–´ë‘” íŒŒì¼ì„ HDFSì— ì˜¬ë¦¬ê³  ì‘ì„±í•´ë‘” MapReduce í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•´ë³´ë ¤ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„  ë¨¼ì € datanode ì»¨í…Œì´ë„ˆ ì•ˆì— ì‘ì„±í•´ë‘” íŒŒì¼ì´ ìˆëŠ” ìœ„ì¹˜ë¡œ ì´ë™ì„ í•´ì•¼í•©ë‹ˆë‹¤.

```bash
docker exec -it docker-hadoop-spark-workbench_datanode_1 /bin/bash
```

```bash
cd /hadoop/dfs/data
ls
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-04 á„‹á…©á„’á…® 4.35.01.png](images/2.5.5.png)

(í•„ìëŠ” ì´ë¯¸ í•œë²ˆ ì‹¤í–‰í–ˆê¸° ë•Œë¬¸ì— class íŒŒì¼ì´ë‘ jar íŒŒì¼ ë“± ì—¬ëŸ¬ ì¡ë‹¤í•œ íŒŒì¼ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤..!)

ì–´ë–¤ ë””ë ‰í† ë¦¬ì—ì„œë“  hadoop ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„  javaì™€ hadoop ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤! ê°„ë‹¨í•˜ê²Œ ë‹¤ìŒì˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ë©´ ë©ë‹ˆë‹¤!

```bash
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
```

ì´ì œ class íŒŒì¼ì„ ë§Œë“¤ê³  ì´ë¥¼ jar íŒŒì¼ë¡œ íŒ¨í‚¤ì§• í•˜ëŠ” ì‘ì—…ë§Œ í•˜ë©´ ì¤€ë¹„ëŠ” ëë‚¬ìŠµë‹ˆë‹¤!

```bash
hadoop com.sun.tools.javac.Main WordCount.java
jar cf wc.jar WordCount*.class
```

- "hadoop com.sun.tools.javac.Main"ì€ Hadoopì—ì„œ ì œê³µí•˜ëŠ” ìë°” ì»´íŒŒì¼ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì»´íŒŒì¼í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.
- "WordCount.java"ëŠ” ì»´íŒŒì¼í•˜ë ¤ëŠ” ìë°” íŒŒì¼ì˜ ì´ë¦„ì…ë‹ˆë‹¤. ì´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ "WordCount.class"ë¼ëŠ” ë°”ì´ë„ˆë¦¬ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ íŒŒì¼ì€ MapReduce ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.

ì! ì´ì œ HDFSì— ì˜¬ë¦´ ì‹œê°„ì…ë‹ˆë‹¤.

```bash
hadoop fs -put ./words.txt /
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-07 á„‹á…©á„Œá…¥á†« 3.05.57.png](images/2.5.6_put.png)

ì´í›„ ì•ì„œ ë§Œë“¤ì–´ë‘” jar íŒŒì¼ì„ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

```bash
hadoop jar wc.jar WordCount /words.txt /output
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-07 á„‹á…©á„Œá…¥á†« 3.03.26.png](images/2.5.7.png)

â†’ ì‘ì—…ì´ ìˆ˜í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤!

output íŒŒì¼ì„ ì—´ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
```bash
hadoop fs -ls /output
hadoop fs -cat /output/part-r-00000
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-05-07 á„‹á…©á„Œá…¥á†« 3.02.48.png](images/2.5.8.png)


<script src="https://utteranc.es/client.js"
        repo="Pseudo-Lab/data-engineering-for-everybody"
        issue-term="pathname"
        label="comments"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>