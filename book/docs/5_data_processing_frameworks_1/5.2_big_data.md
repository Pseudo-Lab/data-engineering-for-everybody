# 5.2 대규모 데이터 처리 필요성과 프레임워크

## 📌 대규모 데이터의 기준

- 기존의 데이터 처리방법으로는 감당하기 힘들 정도로 방대한 분량의 데이터를 뜻합니다.
- 수십 TB ~ 수십 PB 정도의 규모입니다.

## 📌 이 대규모 데이터를 처리하기 위해 필요한 프레임워크로는 어떤 것들이 있을까요?

### Hadoop
- 대용량의 데이터를 분산 처리를 할 수 있는 오픈소스입니다.
- HDFS + MapReduce
    - HDFS: 분산 **저장** 기술
    - MapReduce: 분산 **처리** 기술

### Spark
- **실시간성 데이터에 대한 니즈가 증가**하며 **하둡으로 처리하기에는 속도 측면에서 부적합**한 시나리오들이 등장하기 시작했습니다.
- **분산 시스템을 사용한 프로그래밍 환경**으로 **대량의 메모리를 활용하여 고속화** 실현합니다
- **인메모리상**에서 동작하기 때문에, **반복적인 처리가 필요한 작업에서 속도가 하둡보다 최소 1000배 이상 빠릅니다.**
- 스파크는 Hadoop 이 아니라 MapReduce 를 대체하는 기술입니다.

### Hive
- **HDFS등에 있는 파일을 읽어들여 쿼리로 분석을 수행**합니다.
- Hive는 데이터베이스가 아닌 **데이터 처리를 위한 배치 처리 구조**입니다.
- 대규모 정형 데이터를 처리할 때 매우 안정적입니다.
    

## 🤔 Spark 와 Hive 둘 중 어떤 걸 사용해야 할까요?

- Spark의 경우 **실시간 데이터 처리와 데이터 과학 등의 분석 작업**에 매우 적합합니다.
    - 빠른 처리 속도, 병렬 처리를 위해 분산 컴퓨팅 환경에서 실행합니다.
    - 다양한 언어 지원합니다.
- Hive의 경우 대용량의 정형 데이터를 처리하기에 적합하며, 데이터 마이닝 및 비즈니스 인텔리전스 작업에 적합합니다.
    - SQL 기반의 쿼리 언어 지원합니다.
    - 데이터를 읽고 쓰는 데 많은 시간이 소요되지만, 대규모 데이터를 처리하는 데 효율적이기에 **안정적**입니다.