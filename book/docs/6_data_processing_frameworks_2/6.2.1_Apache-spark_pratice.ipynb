{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `💻 실습: 아파치 스파크`\n",
        "\n",
        "## 파티션, transformation, action 등 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V41AuzIaDU6E"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "VoPmPKjyD11_",
        "outputId": "6eaa43b9-a906-4c47-d02f-f2af8a7ca1bf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b93fea624e9d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EgWmnhcAD4Fu"
      },
      "outputs": [],
      "source": [
        "data = [1,2,3,4,5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M6iZzApDvXz",
        "outputId": "61492d8b-a5ec-485c-bca6-28efbfcd3cf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 파티션의 개수를 설정가능. 일반적으로 스파크는 자동으로 개수 설정\n",
        "distData = sc.parallelize(data, 10)     # 10개의 파티션으로 수행\n",
        "distData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hQHgai1D6ad",
        "outputId": "cb6d364e-5926-4ef5-82ad-92e47d453f6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = distData.reduce(lambda a, b : a + b)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBmn1z8zEG5A"
      },
      "source": [
        "### 외부 데이터셋 가져오기\n",
        "- 로컬 파일, HDFS, S3 등 하둡이 지원하는 스토리지로부터 분산 데이터셋을 불러올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xguYBQxUDwkA"
      },
      "outputs": [],
      "source": [
        "# lines는 현재 메모리에 로드되지 않고 해당 파일을 가르키는 포인터임\n",
        "lines = sc.textFile('./printed.txt',)\n",
        "\n",
        "# map이라는 변환을 취한 후 결과값(연산되지 않는 상태)\n",
        "lineLengths = lines.map(lambda l: len(l)) # PythonRDD[16] at RDD at PythonRDD.scala:53\n",
        "\n",
        "# reduce라는 액션을 취함으로써 병렬 처리를 하면서 작업 연산을 수행\n",
        "# 결과값만 driver program에게 반환\n",
        "totalLength = lineLengths.reduce(lambda a, b: a+b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep learning \n",
        "(image classification) - 일부 예시입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from sparkdl import DeepImageFeaturizer\n",
        "\n",
        "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
        "p = Pipeline(stages=[featurizer, lr])\n",
        "\n",
        "model = p.fit(train_images_df)    # train_images_df is a dataset of images and labels\n",
        "\n",
        "# Inspect training error\n",
        "df = model.transform(train_images_df.limit(10)).select(\"image\", \"probability\",  \"uri\", \"label\")\n",
        "predictionAndLabels = df.select(\"prediction\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(\"Training set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
