{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💻 실습: Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Airflow 관련 폴더 및 유저 권한 설정\n",
    "mkdir -p ./dags ./logs ./plugins\n",
    "echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 참고사항 - CLI 설정\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.5.1/airflow.sh'\n",
    "chmod +x airflow.sh\n",
    "\n",
    "# 설치 정보 확인\n",
    "./airflow.sh airflow info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1️⃣ hello_world.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def helloworld():\n",
    "    print(\"hello world\")\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"hello_world_dag\",\n",
    "    start_date=datetime(2023,5,25),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False\n",
    "    ) as dag:\n",
    "    \n",
    "    task1 = PythonOperator(\n",
    "        task_id = \"hello_world\",\n",
    "        python_callable=helloworld\n",
    "    )\n",
    "\n",
    "task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Airflow 실행\n",
    "# 처음 실행 시, 2분 내로 실행\n",
    "docker-compose up -d\n",
    "\n",
    "# 접속\n",
    "localhost:8080\n",
    "- ID: airflow\n",
    "- PW: airflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 사용 리소스 정리\n",
    "docker-compose down --volumes --rmi all\n",
    "\n",
    "# 작업 폴더 삭제\n",
    "cd .. && rm -r airflow-docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2️⃣ download_rocket_launches.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 작업 폴더 생성\n",
    "mkdir -p airflow-docker/dags && cd airflow-docker\n",
    "\n",
    "# docker-compose.yaml 파일 생성\n",
    "cat <<EOF > docker-compose.yaml\n",
    "version: '3.7'\n",
    "# ====================================== AIRFLOW ENVIRONMENT VARIABLES =======================================\n",
    "x-environment: &airflow_environment\n",
    "  - AIRFLOW__CORE__EXECUTOR=LocalExecutor\n",
    "  - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False\n",
    "  - AIRFLOW__CORE__LOAD_EXAMPLES=False\n",
    "  - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow\n",
    "  - AIRFLOW__CORE__STORE_DAG_CODE=True\n",
    "  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True\n",
    "  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True\n",
    "\n",
    "x-airflow-image: &airflow_image apache/airflow:2.0.0-python3.8\n",
    "# ====================================== /AIRFLOW ENVIRONMENT VARIABLES ======================================\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:12-alpine\n",
    "    environment:\n",
    "      - POSTGRES_USER=airflow\n",
    "      - POSTGRES_PASSWORD=airflow\n",
    "      - POSTGRES_DB=airflow\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "\n",
    "  init:\n",
    "    image: *airflow_image\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    environment: *airflow_environment\n",
    "    entrypoint: /bin/bash\n",
    "    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org'\n",
    "\n",
    "  webserver:\n",
    "    image: *airflow_image\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    volumes:\n",
    "      - logs:/opt/airflow/logs\n",
    "    environment: *airflow_environment\n",
    "    command: webserver\n",
    "\n",
    "  scheduler:\n",
    "    image: *airflow_image\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    volumes:\n",
    "      - logs:/opt/airflow/logs\n",
    "      - ./dags:/opt/airflow/dags\n",
    "    environment: *airflow_environment\n",
    "    command: scheduler\n",
    "\n",
    "volumes:\n",
    "  logs:\n",
    "EOF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "import airflow.utils.dates\n",
    "import requests\n",
    "import requests.exceptions as requests_exceptions\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"download_rocket_launches\",\n",
    "    description=\"Download rocket pictures of recently launched rockets.\",\n",
    "    start_date=airflow.utils.dates.days_ago(14),\n",
    "    schedule_interval=\"@daily\",\n",
    ")\n",
    "\n",
    "download_launches = BashOperator(\n",
    "    task_id=\"download_launches\",\n",
    "    bash_command=\"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\",  # noqa: E501\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "def _get_pictures():\n",
    "    # Ensure directory exists\n",
    "    pathlib.Path(\"/tmp/images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download all pictures in launches.json\n",
    "    with open(\"/tmp/launches.json\") as f:\n",
    "        launches = json.load(f)\n",
    "        image_urls = [launch[\"image\"] for launch in launches[\"results\"]]\n",
    "        for image_url in image_urls:\n",
    "            try:\n",
    "                response = requests.get(image_url)\n",
    "                image_filename = image_url.split(\"/\")[-1]\n",
    "                target_file = f\"/tmp/images/{image_filename}\"\n",
    "                with open(target_file, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded {image_url} to {target_file}\")\n",
    "            except requests_exceptions.MissingSchema:\n",
    "                print(f\"{image_url} appears to be an invalid URL.\")\n",
    "            except requests_exceptions.ConnectionError:\n",
    "                print(f\"Could not connect to {image_url}.\")\n",
    "\n",
    "\n",
    "get_pictures = PythonOperator(\n",
    "    task_id=\"get_pictures\", python_callable=_get_pictures, dag=dag\n",
    ")\n",
    "\n",
    "notify = BashOperator(\n",
    "    task_id=\"notify\",\n",
    "    bash_command='echo \"There are now $(ls /tmp/images/ | wc -l) images.\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "download_launches >> get_pictures >> notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 아래의 부분 중, ls~ 는 실제 경로가 없으므로 파일 내에서 수정\n",
    "# bash_command='echo \"There are now $(ls /tmp/images/ | wc -l) images.\"',\n",
    "\n",
    "# 실행\n",
    "docker-compose up -d\n",
    "\n",
    "# 접속\n",
    "localhost:8080\n",
    "ID: admin\n",
    "PW: admin\n",
    "\n",
    "# 실습 완료 후, 리소스 삭제\n",
    "docker-compose down\n",
    "cd .. && rm -r airflow-docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ 추가 예시 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로그 전처리 및 전달 (로그 서버 및 샘플 데이터 필요)\n",
    "\n",
    "[Step by step: build a data pipeline with Airflow](https://towardsdatascience.com/step-by-step-build-a-data-pipeline-with-airflow-4f96854f7466)\n",
    "\n",
    "[GitHub - kyokin78/airflow: Build a data pipeline with Apache Airflow](https://github.com/kyokin78/airflow/tree/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로그 처리 과정\n",
    "\n",
    "<img src=\"./images/7_4_1.png\">\n",
    "\n",
    "### DAG 구성\n",
    "\n",
    "<img src=\"./images/7_4_2.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
