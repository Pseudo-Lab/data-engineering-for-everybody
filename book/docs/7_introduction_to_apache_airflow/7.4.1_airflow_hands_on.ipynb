{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ’» ì‹¤ìŠµ: Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Airflow ê´€ë ¨ í´ë” ë° ìœ ì € ê¶Œí•œ ì„¤ì •\n",
    "mkdir -p ./dags ./logs ./plugins\n",
    "echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# ì°¸ê³ ì‚¬í•­ - CLI ì„¤ì •\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.5.1/airflow.sh'\n",
    "chmod +x airflow.sh\n",
    "\n",
    "# ì„¤ì¹˜ ì •ë³´ í™•ì¸\n",
    "./airflow.sh airflow info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ 1ï¸âƒ£ hello_world.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def helloworld():\n",
    "    print(\"hello world\")\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"hello_world_dag\",\n",
    "    start_date=datetime(2023,5,25),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False\n",
    "    ) as dag:\n",
    "    \n",
    "    task1 = PythonOperator(\n",
    "        task_id = \"hello_world\",\n",
    "        python_callable=helloworld\n",
    "    )\n",
    "\n",
    "task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Airflow ì‹¤í–‰\n",
    "# ì²˜ìŒ ì‹¤í–‰ ì‹œ, 2ë¶„ ë‚´ë¡œ ì‹¤í–‰\n",
    "docker-compose up -d\n",
    "\n",
    "# ì ‘ì†\n",
    "localhost:8080\n",
    "- ID: airflow\n",
    "- PW: airflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# ì‚¬ìš© ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n",
    "docker-compose down --volumes --rmi all\n",
    "\n",
    "# ì‘ì—… í´ë” ì‚­ì œ\n",
    "cd .. && rm -r airflow-docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ 2ï¸âƒ£ download_rocket_launches.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# ì‘ì—… í´ë” ìƒì„±\n",
    "mkdir -p airflow-docker/dags && cd airflow-docker\n",
    "\n",
    "# docker-compose.yaml íŒŒì¼ ìƒì„±\n",
    "cat <<EOF > docker-compose.yaml\n",
    "version: '3.7'\n",
    "# ====================================== AIRFLOW ENVIRONMENT VARIABLES =======================================\n",
    "x-environment: &airflow_environment\n",
    "  - AIRFLOW__CORE__EXECUTOR=LocalExecutor\n",
    "  - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False\n",
    "  - AIRFLOW__CORE__LOAD_EXAMPLES=False\n",
    "  - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow\n",
    "  - AIRFLOW__CORE__STORE_DAG_CODE=True\n",
    "  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True\n",
    "  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True\n",
    "\n",
    "x-airflow-image: &airflow_image apache/airflow:2.0.0-python3.8\n",
    "# ====================================== /AIRFLOW ENVIRONMENT VARIABLES ======================================\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:12-alpine\n",
    "    environment:\n",
    "      - POSTGRES_USER=airflow\n",
    "      - POSTGRES_PASSWORD=airflow\n",
    "      - POSTGRES_DB=airflow\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "\n",
    "  init:\n",
    "    image: *airflow_image\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    environment: *airflow_environment\n",
    "    entrypoint: /bin/bash\n",
    "    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org'\n",
    "\n",
    "  webserver:\n",
    "    image: *airflow_image\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    volumes:\n",
    "      - logs:/opt/airflow/logs\n",
    "    environment: *airflow_environment\n",
    "    command: webserver\n",
    "\n",
    "  scheduler:\n",
    "    image: *airflow_image\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    volumes:\n",
    "      - logs:/opt/airflow/logs\n",
    "      - ./dags:/opt/airflow/dags\n",
    "    environment: *airflow_environment\n",
    "    command: scheduler\n",
    "\n",
    "volumes:\n",
    "  logs:\n",
    "EOF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "import airflow.utils.dates\n",
    "import requests\n",
    "import requests.exceptions as requests_exceptions\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"download_rocket_launches\",\n",
    "    description=\"Download rocket pictures of recently launched rockets.\",\n",
    "    start_date=airflow.utils.dates.days_ago(14),\n",
    "    schedule_interval=\"@daily\",\n",
    ")\n",
    "\n",
    "download_launches = BashOperator(\n",
    "    task_id=\"download_launches\",\n",
    "    bash_command=\"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\",  # noqa: E501\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "def _get_pictures():\n",
    "    # Ensure directory exists\n",
    "    pathlib.Path(\"/tmp/images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download all pictures in launches.json\n",
    "    with open(\"/tmp/launches.json\") as f:\n",
    "        launches = json.load(f)\n",
    "        image_urls = [launch[\"image\"] for launch in launches[\"results\"]]\n",
    "        for image_url in image_urls:\n",
    "            try:\n",
    "                response = requests.get(image_url)\n",
    "                image_filename = image_url.split(\"/\")[-1]\n",
    "                target_file = f\"/tmp/images/{image_filename}\"\n",
    "                with open(target_file, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded {image_url} to {target_file}\")\n",
    "            except requests_exceptions.MissingSchema:\n",
    "                print(f\"{image_url} appears to be an invalid URL.\")\n",
    "            except requests_exceptions.ConnectionError:\n",
    "                print(f\"Could not connect to {image_url}.\")\n",
    "\n",
    "\n",
    "get_pictures = PythonOperator(\n",
    "    task_id=\"get_pictures\", python_callable=_get_pictures, dag=dag\n",
    ")\n",
    "\n",
    "notify = BashOperator(\n",
    "    task_id=\"notify\",\n",
    "    bash_command='echo \"There are now $(ls /tmp/images/ | wc -l) images.\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "download_launches >> get_pictures >> notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# ì•„ë˜ì˜ ë¶€ë¶„ ì¤‘, ls~ ëŠ” ì‹¤ì œ ê²½ë¡œê°€ ì—†ìœ¼ë¯€ë¡œ íŒŒì¼ ë‚´ì—ì„œ ìˆ˜ì •\n",
    "# bash_command='echo \"There are now $(ls /tmp/images/ | wc -l) images.\"',\n",
    "\n",
    "# ì‹¤í–‰\n",
    "docker-compose up -d\n",
    "\n",
    "# ì ‘ì†\n",
    "localhost:8080\n",
    "ID: admin\n",
    "PW: admin\n",
    "\n",
    "# ì‹¤ìŠµ ì™„ë£Œ í›„, ë¦¬ì†ŒìŠ¤ ì‚­ì œ\n",
    "docker-compose down\n",
    "cd .. && rm -r airflow-docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ì¶”ê°€ ì˜ˆì‹œ ì†Œê°œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¡œê·¸ ì „ì²˜ë¦¬ ë° ì „ë‹¬ (ë¡œê·¸ ì„œë²„ ë° ìƒ˜í”Œ ë°ì´í„° í•„ìš”)\n",
    "\n",
    "[Step by step: build a data pipeline with Airflow](https://towardsdatascience.com/step-by-step-build-a-data-pipeline-with-airflow-4f96854f7466)\n",
    "\n",
    "[GitHub - kyokin78/airflow: Build a data pipeline with Apache Airflow](https://github.com/kyokin78/airflow/tree/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¡œê·¸ ì²˜ë¦¬ ê³¼ì •\n",
    "\n",
    "<img src=\"./images/7_4_1.png\">\n",
    "\n",
    "### DAG êµ¬ì„±\n",
    "\n",
    "<img src=\"./images/7_4_2.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
